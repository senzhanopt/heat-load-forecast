{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f457b2a2-b323-4422-b678-93e8d89f7891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow import keras\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca999436-5227-43ae-b000-57249bce0129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     X1     X2     X3      X4   X5  X6   X7  X8     Y1     Y2\n",
      "0  0.98  514.5  294.0  110.25  7.0   2  0.0   0  15.55  21.33\n",
      "1  0.98  514.5  294.0  110.25  7.0   3  0.0   0  15.55  21.33\n",
      "2  0.98  514.5  294.0  110.25  7.0   4  0.0   0  15.55  21.33\n",
      "3  0.98  514.5  294.0  110.25  7.0   5  0.0   0  15.55  21.33\n",
      "4  0.90  563.5  318.5  122.50  7.0   2  0.0   0  20.84  28.28\n"
     ]
    }
   ],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\"\n",
    "data = pd.read_excel(url)\n",
    "\n",
    "print(data.head())\n",
    "# print(data.info())\n",
    "# print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff856053-49d3-4a6d-990b-2f5b349a2b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-2]\n",
    "y = data['Y1']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test  = X_train.to_numpy(), X_test.to_numpy(), y_train.to_numpy(), y_test.to_numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95d6c86d-9d18-4e04-af8e-28e2957b5f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000/\")\n",
    "mlflow.set_experiment(\"heat_load_prediction\")\n",
    "model_name = \"heat-forecast-prod\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54b7f4b4-36c3-4819-84b5-e2052990fbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/08 11:25:32 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒ View run funny-hen-961 at: http://127.0.0.1:5000/#/experiments/195097301821250000/runs/076e7dbb2bea4c78b4668d492fdb5e56\n",
      "ğŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/195097301821250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/08 11:25:35 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒ View run debonair-flea-460 at: http://127.0.0.1:5000/#/experiments/195097301821250000/runs/89370e334f3749249bc3024aced7f671\n",
      "ğŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/195097301821250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/08 11:25:40 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒ View run colorful-asp-495 at: http://127.0.0.1:5000/#/experiments/195097301821250000/runs/705a287cf4ff4058b546f07129c8f522\n",
      "ğŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/195097301821250000\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"XGBoost Regressor\": xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "for name, model in models.items():\n",
    "    with mlflow.start_run():\n",
    "\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        mlflow.log_param(\"model_name\", name)\n",
    "        mlflow.log_metric(\"mse\", mse)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "        mlflow.sklearn.log_model(model, name = model_name, input_example = X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "772e34c2-72bc-4e3f-afa4-d2f78b6ce35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, val_loss=549.8357\n",
      "Epoch 2, val_loss=516.3857\n",
      "Epoch 3, val_loss=461.0833\n",
      "Epoch 4, val_loss=380.1158\n",
      "Epoch 5, val_loss=277.0799\n",
      "Epoch 6, val_loss=168.3986\n",
      "Epoch 7, val_loss=82.8788\n",
      "Epoch 8, val_loss=42.3065\n",
      "Epoch 9, val_loss=32.1516\n",
      "Epoch 10, val_loss=27.7435\n",
      "Epoch 11, val_loss=24.7876\n",
      "Epoch 12, val_loss=22.7668\n",
      "Epoch 13, val_loss=21.6246\n",
      "Epoch 14, val_loss=20.9114\n",
      "Epoch 15, val_loss=19.9821\n",
      "Epoch 16, val_loss=19.2694\n",
      "Epoch 17, val_loss=18.6336\n",
      "Epoch 18, val_loss=18.1524\n",
      "Epoch 19, val_loss=17.5056\n",
      "Epoch 20, val_loss=16.9990\n",
      "Epoch 21, val_loss=16.8271\n",
      "Epoch 22, val_loss=15.8893\n",
      "Epoch 23, val_loss=15.7215\n",
      "Epoch 24, val_loss=15.4434\n",
      "Epoch 25, val_loss=14.7488\n",
      "Epoch 26, val_loss=14.4586\n",
      "Epoch 27, val_loss=14.1273\n",
      "Epoch 28, val_loss=13.7918\n",
      "Epoch 29, val_loss=13.3930\n",
      "Epoch 30, val_loss=13.1360\n",
      "Epoch 31, val_loss=12.9396\n",
      "Epoch 32, val_loss=12.5319\n",
      "Epoch 33, val_loss=12.3250\n",
      "Epoch 34, val_loss=11.7853\n",
      "Epoch 35, val_loss=11.7373\n",
      "Epoch 36, val_loss=11.6139\n",
      "Epoch 37, val_loss=11.2994\n",
      "Epoch 38, val_loss=11.2530\n",
      "Epoch 39, val_loss=10.7870\n",
      "Epoch 40, val_loss=10.7941\n",
      "Epoch 41, val_loss=10.4274\n",
      "Epoch 42, val_loss=10.3038\n",
      "Epoch 43, val_loss=10.2947\n",
      "Epoch 44, val_loss=10.1267\n",
      "Epoch 45, val_loss=9.9672\n",
      "Epoch 46, val_loss=10.0264\n",
      "Epoch 47, val_loss=9.8240\n",
      "Epoch 48, val_loss=9.6479\n",
      "Epoch 49, val_loss=9.6351\n",
      "Epoch 50, val_loss=9.5895\n",
      "Epoch 51, val_loss=9.3301\n",
      "Epoch 52, val_loss=9.3259\n",
      "Epoch 53, val_loss=9.2736\n",
      "Epoch 54, val_loss=9.1537\n",
      "Epoch 55, val_loss=9.1963\n",
      "Epoch 56, val_loss=9.0434\n",
      "Epoch 57, val_loss=8.9267\n",
      "Epoch 58, val_loss=8.9180\n",
      "Epoch 59, val_loss=8.8995\n",
      "Epoch 60, val_loss=8.8451\n",
      "Epoch 61, val_loss=8.6057\n",
      "Epoch 62, val_loss=8.7060\n",
      "Epoch 63, val_loss=8.5272\n",
      "Epoch 64, val_loss=8.7080\n",
      "Epoch 65, val_loss=8.5182\n",
      "Epoch 66, val_loss=8.4250\n",
      "Epoch 67, val_loss=8.4388\n",
      "Epoch 68, val_loss=8.3480\n",
      "Epoch 69, val_loss=8.2574\n",
      "Epoch 70, val_loss=8.2121\n",
      "Epoch 71, val_loss=8.1281\n",
      "Epoch 72, val_loss=8.0434\n",
      "Epoch 73, val_loss=8.0536\n",
      "Epoch 74, val_loss=7.9197\n",
      "Epoch 75, val_loss=7.9680\n",
      "Epoch 76, val_loss=7.8437\n",
      "Epoch 77, val_loss=7.8733\n",
      "Epoch 78, val_loss=7.7269\n",
      "Epoch 79, val_loss=7.6613\n",
      "Epoch 80, val_loss=7.7081\n",
      "Epoch 81, val_loss=7.5776\n",
      "Epoch 82, val_loss=7.5764\n",
      "Epoch 83, val_loss=7.4751\n",
      "Epoch 84, val_loss=7.4423\n",
      "Epoch 85, val_loss=7.3867\n",
      "Epoch 86, val_loss=7.3220\n",
      "Epoch 87, val_loss=7.2364\n",
      "Epoch 88, val_loss=7.1412\n",
      "Epoch 89, val_loss=7.2651\n",
      "Epoch 90, val_loss=7.1248\n",
      "Epoch 91, val_loss=6.9713\n",
      "Epoch 92, val_loss=7.0105\n",
      "Epoch 93, val_loss=6.9323\n",
      "Epoch 94, val_loss=6.9729\n",
      "Epoch 95, val_loss=6.8288\n",
      "Epoch 96, val_loss=6.9018\n",
      "Epoch 97, val_loss=6.7798\n",
      "Epoch 98, val_loss=6.8344\n",
      "Epoch 99, val_loss=6.6369\n",
      "Epoch 100, val_loss=6.6585\n",
      "Epoch 101, val_loss=6.6297\n",
      "Epoch 102, val_loss=6.5931\n",
      "Epoch 103, val_loss=6.5489\n",
      "Epoch 104, val_loss=6.5416\n",
      "Epoch 105, val_loss=6.5245\n",
      "Epoch 106, val_loss=6.4573\n",
      "Epoch 107, val_loss=6.4898\n",
      "Epoch 108, val_loss=6.2730\n",
      "Epoch 109, val_loss=6.5049\n",
      "Epoch 110, val_loss=6.1763\n",
      "Epoch 111, val_loss=6.2339\n",
      "Epoch 112, val_loss=6.2283\n",
      "Epoch 113, val_loss=6.2418\n",
      "Epoch 114, val_loss=6.1516\n",
      "Epoch 115, val_loss=6.0928\n",
      "Epoch 116, val_loss=6.1019\n",
      "Epoch 117, val_loss=6.0207\n",
      "Epoch 118, val_loss=6.1005\n",
      "Epoch 119, val_loss=6.0020\n",
      "Epoch 120, val_loss=6.0730\n",
      "Epoch 121, val_loss=5.8748\n",
      "Epoch 122, val_loss=5.9048\n",
      "Epoch 123, val_loss=5.7276\n",
      "Epoch 124, val_loss=5.7838\n",
      "Epoch 125, val_loss=5.7653\n",
      "Epoch 126, val_loss=5.7676\n",
      "Epoch 127, val_loss=5.6733\n",
      "Epoch 128, val_loss=5.7586\n",
      "Epoch 129, val_loss=5.6202\n",
      "Epoch 130, val_loss=5.7695\n",
      "Epoch 131, val_loss=5.5545\n",
      "Epoch 132, val_loss=5.7727\n",
      "Epoch 133, val_loss=5.5190\n",
      "Epoch 134, val_loss=5.5373\n",
      "Epoch 135, val_loss=5.5685\n",
      "Epoch 136, val_loss=5.6380\n",
      "Epoch 137, val_loss=5.3858\n",
      "Epoch 138, val_loss=5.4722\n",
      "Epoch 139, val_loss=5.4451\n",
      "Epoch 140, val_loss=5.3260\n",
      "Epoch 141, val_loss=5.3654\n",
      "Epoch 142, val_loss=5.3160\n",
      "Epoch 143, val_loss=5.2856\n",
      "Epoch 144, val_loss=5.5214\n",
      "Epoch 145, val_loss=5.3136\n",
      "Epoch 146, val_loss=5.2191\n",
      "Epoch 147, val_loss=5.2514\n",
      "Epoch 148, val_loss=5.2257\n",
      "Epoch 149, val_loss=5.2375\n",
      "Epoch 150, val_loss=5.0637\n",
      "Epoch 151, val_loss=5.1506\n",
      "Epoch 152, val_loss=5.0873\n",
      "Epoch 153, val_loss=5.1596\n",
      "Epoch 154, val_loss=5.1209\n",
      "Epoch 155, val_loss=5.0869\n",
      "Epoch 156, val_loss=4.9587\n",
      "Epoch 157, val_loss=4.9854\n",
      "Epoch 158, val_loss=4.9858\n",
      "Epoch 159, val_loss=5.3316\n",
      "Epoch 160, val_loss=4.9197\n",
      "Epoch 161, val_loss=4.8626\n",
      "Epoch 162, val_loss=5.0315\n",
      "Epoch 163, val_loss=4.8223\n",
      "Epoch 164, val_loss=5.0023\n",
      "Epoch 165, val_loss=4.7721\n",
      "Epoch 166, val_loss=5.2340\n",
      "Epoch 167, val_loss=5.0859\n",
      "Epoch 168, val_loss=4.6718\n",
      "Epoch 169, val_loss=5.0473\n",
      "Epoch 170, val_loss=4.6716\n",
      "Epoch 171, val_loss=4.6070\n",
      "Epoch 172, val_loss=4.7537\n",
      "Epoch 173, val_loss=4.5896\n",
      "Epoch 174, val_loss=4.6197\n",
      "Epoch 175, val_loss=4.5448\n",
      "Epoch 176, val_loss=4.4285\n",
      "Epoch 177, val_loss=4.3450\n",
      "Epoch 178, val_loss=4.3914\n",
      "Epoch 179, val_loss=4.6268\n",
      "Epoch 180, val_loss=4.3195\n",
      "Epoch 181, val_loss=4.3617\n",
      "Epoch 182, val_loss=4.1401\n",
      "Epoch 183, val_loss=4.1936\n",
      "Epoch 184, val_loss=4.1248\n",
      "Epoch 185, val_loss=4.1261\n",
      "Epoch 186, val_loss=4.2888\n",
      "Epoch 187, val_loss=3.9478\n",
      "Epoch 188, val_loss=3.8988\n",
      "Epoch 189, val_loss=3.9243\n",
      "Epoch 190, val_loss=3.8995\n",
      "Epoch 191, val_loss=3.9034\n",
      "Epoch 192, val_loss=3.7953\n",
      "Epoch 193, val_loss=3.7340\n",
      "Epoch 194, val_loss=3.6011\n",
      "Epoch 195, val_loss=3.6617\n",
      "Epoch 196, val_loss=3.4767\n",
      "Epoch 197, val_loss=3.4473\n",
      "Epoch 198, val_loss=3.4506\n",
      "Epoch 199, val_loss=3.3178\n",
      "Epoch 200, val_loss=3.2782\n",
      "Epoch 201, val_loss=3.2844\n",
      "Epoch 202, val_loss=3.0857\n",
      "Epoch 203, val_loss=3.2218\n",
      "Epoch 204, val_loss=2.9965\n",
      "Epoch 205, val_loss=2.9571\n",
      "Epoch 206, val_loss=2.9162\n",
      "Epoch 207, val_loss=2.9192\n",
      "Epoch 208, val_loss=2.8079\n",
      "Epoch 209, val_loss=2.7492\n",
      "Epoch 210, val_loss=2.8525\n",
      "Epoch 211, val_loss=2.6452\n",
      "Epoch 212, val_loss=2.7181\n",
      "Epoch 213, val_loss=2.6339\n",
      "Epoch 214, val_loss=2.5775\n",
      "Epoch 215, val_loss=2.4607\n",
      "Epoch 216, val_loss=2.4760\n",
      "Epoch 217, val_loss=2.3748\n",
      "Epoch 218, val_loss=2.7452\n",
      "Epoch 219, val_loss=2.4940\n",
      "Epoch 220, val_loss=2.2454\n",
      "Epoch 221, val_loss=2.2423\n",
      "Epoch 222, val_loss=2.1332\n",
      "Epoch 223, val_loss=2.2533\n",
      "Epoch 224, val_loss=2.0968\n",
      "Epoch 225, val_loss=2.0643\n",
      "Epoch 226, val_loss=2.0056\n",
      "Epoch 227, val_loss=1.9637\n",
      "Epoch 228, val_loss=1.9728\n",
      "Epoch 229, val_loss=2.0790\n",
      "Epoch 230, val_loss=1.8780\n",
      "Epoch 231, val_loss=1.8806\n",
      "Epoch 232, val_loss=1.8229\n",
      "Epoch 233, val_loss=1.7942\n",
      "Epoch 234, val_loss=1.8114\n",
      "Epoch 235, val_loss=1.6692\n",
      "Epoch 236, val_loss=1.6464\n",
      "Epoch 237, val_loss=1.6409\n",
      "Epoch 238, val_loss=1.6004\n",
      "Epoch 239, val_loss=1.6404\n",
      "Epoch 240, val_loss=1.5353\n",
      "Epoch 241, val_loss=1.4960\n",
      "Epoch 242, val_loss=1.4923\n",
      "Epoch 243, val_loss=1.4460\n",
      "Epoch 244, val_loss=1.4600\n",
      "Epoch 245, val_loss=1.4283\n",
      "Epoch 246, val_loss=1.3671\n",
      "Epoch 247, val_loss=1.3549\n",
      "Epoch 248, val_loss=1.2891\n",
      "Epoch 249, val_loss=1.2896\n",
      "Epoch 250, val_loss=1.2867\n",
      "Epoch 251, val_loss=1.2699\n",
      "Epoch 252, val_loss=1.2521\n",
      "Epoch 253, val_loss=1.2097\n",
      "Epoch 254, val_loss=1.2043\n",
      "Epoch 255, val_loss=1.1611\n",
      "Epoch 256, val_loss=1.1413\n",
      "Epoch 257, val_loss=1.1199\n",
      "Epoch 258, val_loss=1.0989\n",
      "Epoch 259, val_loss=1.0558\n",
      "Epoch 260, val_loss=1.0553\n",
      "Epoch 261, val_loss=1.0502\n",
      "Epoch 262, val_loss=1.1069\n",
      "Epoch 263, val_loss=1.0239\n",
      "Epoch 264, val_loss=1.0071\n",
      "Epoch 265, val_loss=0.9498\n",
      "Epoch 266, val_loss=0.9595\n",
      "Epoch 267, val_loss=0.9234\n",
      "Epoch 268, val_loss=0.9659\n",
      "Epoch 269, val_loss=0.9239\n",
      "Epoch 270, val_loss=0.8937\n",
      "Epoch 271, val_loss=0.9062\n",
      "Epoch 272, val_loss=0.8662\n",
      "Epoch 273, val_loss=0.8440\n",
      "Epoch 274, val_loss=0.8376\n",
      "Epoch 275, val_loss=0.8184\n",
      "Epoch 276, val_loss=0.8352\n",
      "Epoch 277, val_loss=0.7838\n",
      "Epoch 278, val_loss=0.7864\n",
      "Epoch 279, val_loss=0.7669\n",
      "Epoch 280, val_loss=0.7502\n",
      "Epoch 281, val_loss=0.7550\n",
      "Epoch 282, val_loss=0.7238\n",
      "Epoch 283, val_loss=0.7330\n",
      "Epoch 284, val_loss=0.7020\n",
      "Epoch 285, val_loss=0.7114\n",
      "Epoch 286, val_loss=0.7048\n",
      "Epoch 287, val_loss=0.6631\n",
      "Epoch 288, val_loss=0.6528\n",
      "Epoch 289, val_loss=0.6542\n",
      "Epoch 290, val_loss=0.6584\n",
      "Epoch 291, val_loss=0.6470\n",
      "Epoch 292, val_loss=0.6526\n",
      "Epoch 293, val_loss=0.6042\n",
      "Epoch 294, val_loss=0.6009\n",
      "Epoch 295, val_loss=0.6264\n",
      "Epoch 296, val_loss=0.5833\n",
      "Epoch 297, val_loss=0.5974\n",
      "Epoch 298, val_loss=0.6017\n",
      "Epoch 299, val_loss=0.5821\n",
      "Epoch 300, val_loss=0.5609\n",
      "Epoch 301, val_loss=0.5426\n",
      "Epoch 302, val_loss=0.5427\n",
      "Epoch 303, val_loss=0.5356\n",
      "Epoch 304, val_loss=0.5287\n",
      "Epoch 305, val_loss=0.5218\n",
      "Epoch 306, val_loss=0.4980\n",
      "Epoch 307, val_loss=0.5086\n",
      "Epoch 308, val_loss=0.4984\n",
      "Epoch 309, val_loss=0.4908\n",
      "Epoch 310, val_loss=0.4705\n",
      "Epoch 311, val_loss=0.4920\n",
      "Epoch 312, val_loss=0.4940\n",
      "Epoch 313, val_loss=0.4623\n",
      "Epoch 314, val_loss=0.4502\n",
      "Epoch 315, val_loss=0.4586\n",
      "Epoch 316, val_loss=0.5021\n",
      "Epoch 317, val_loss=0.4478\n",
      "Epoch 318, val_loss=0.4552\n",
      "Epoch 319, val_loss=0.5214\n",
      "Epoch 320, val_loss=0.4758\n",
      "Epoch 321, val_loss=0.5918\n",
      "Epoch 322, val_loss=0.4372\n",
      "Epoch 323, val_loss=0.4128\n",
      "Epoch 324, val_loss=0.4126\n",
      "Epoch 325, val_loss=0.4292\n",
      "Epoch 326, val_loss=0.4595\n",
      "Epoch 327, val_loss=0.3855\n",
      "Epoch 328, val_loss=0.3848\n",
      "Epoch 329, val_loss=0.4011\n",
      "Epoch 330, val_loss=0.3927\n",
      "Epoch 331, val_loss=0.3954\n",
      "Epoch 332, val_loss=0.3716\n",
      "Epoch 333, val_loss=0.3555\n",
      "Epoch 334, val_loss=0.3745\n",
      "Epoch 335, val_loss=0.3917\n",
      "Epoch 336, val_loss=0.3575\n",
      "Epoch 337, val_loss=0.3424\n",
      "Epoch 338, val_loss=0.3659\n",
      "Epoch 339, val_loss=0.3513\n",
      "Epoch 340, val_loss=0.3391\n",
      "Epoch 341, val_loss=0.3583\n",
      "Epoch 342, val_loss=0.3299\n",
      "Epoch 343, val_loss=0.3226\n",
      "Epoch 344, val_loss=0.3805\n",
      "Epoch 345, val_loss=0.3599\n",
      "Epoch 346, val_loss=0.3612\n",
      "Epoch 347, val_loss=0.3659\n",
      "Epoch 348, val_loss=0.3393\n",
      "Epoch 349, val_loss=0.3140\n",
      "Epoch 350, val_loss=0.3070\n",
      "Epoch 351, val_loss=0.3263\n",
      "Epoch 352, val_loss=0.3027\n",
      "Epoch 353, val_loss=0.3133\n",
      "Epoch 354, val_loss=0.2998\n",
      "Epoch 355, val_loss=0.2918\n",
      "Epoch 356, val_loss=0.2972\n",
      "Epoch 357, val_loss=0.3164\n",
      "Epoch 358, val_loss=0.3000\n",
      "Epoch 359, val_loss=0.2960\n",
      "Epoch 360, val_loss=0.3245\n",
      "Epoch 361, val_loss=0.2962\n",
      "Epoch 362, val_loss=0.3039\n",
      "Epoch 363, val_loss=0.2899\n",
      "Epoch 364, val_loss=0.2883\n",
      "Epoch 365, val_loss=0.3359\n",
      "Epoch 366, val_loss=0.2927\n",
      "Epoch 367, val_loss=0.2779\n",
      "Epoch 368, val_loss=0.2921\n",
      "Epoch 369, val_loss=0.2999\n",
      "Epoch 370, val_loss=0.3317\n",
      "Epoch 371, val_loss=0.3112\n",
      "Epoch 372, val_loss=0.3365\n",
      "Epoch 373, val_loss=0.2923\n",
      "Epoch 374, val_loss=0.2946\n",
      "Epoch 375, val_loss=0.2624\n",
      "Epoch 376, val_loss=0.2541\n",
      "Epoch 377, val_loss=0.2778\n",
      "Epoch 378, val_loss=0.2605\n",
      "Epoch 379, val_loss=0.2865\n",
      "Epoch 380, val_loss=0.2784\n",
      "Epoch 381, val_loss=0.2679\n",
      "Epoch 382, val_loss=0.2636\n",
      "Epoch 383, val_loss=0.2494\n",
      "Epoch 384, val_loss=0.2717\n",
      "Epoch 385, val_loss=0.2613\n",
      "Epoch 386, val_loss=0.2482\n",
      "Epoch 387, val_loss=0.2572\n",
      "Epoch 388, val_loss=0.2812\n",
      "Epoch 389, val_loss=0.2460\n",
      "Epoch 390, val_loss=0.2601\n",
      "Epoch 391, val_loss=0.2819\n",
      "Epoch 392, val_loss=0.2423\n",
      "Epoch 393, val_loss=0.2447\n",
      "Epoch 394, val_loss=0.2517\n",
      "Epoch 395, val_loss=0.2530\n",
      "Epoch 396, val_loss=0.2497\n",
      "Epoch 397, val_loss=0.2440\n",
      "Epoch 398, val_loss=0.2467\n",
      "Epoch 399, val_loss=0.2454\n",
      "Epoch 400, val_loss=0.2463\n",
      "Epoch 401, val_loss=0.2775\n",
      "Epoch 402, val_loss=0.2427\n",
      "Epoch 403, val_loss=0.2816\n",
      "Epoch 404, val_loss=0.2827\n",
      "Epoch 405, val_loss=0.2577\n",
      "Epoch 406, val_loss=0.2355\n",
      "Epoch 407, val_loss=0.2425\n",
      "Epoch 408, val_loss=0.2387\n",
      "Epoch 409, val_loss=0.2333\n",
      "Epoch 410, val_loss=0.2300\n",
      "Epoch 411, val_loss=0.2462\n",
      "Epoch 412, val_loss=0.2517\n",
      "Epoch 413, val_loss=0.2410\n",
      "Epoch 414, val_loss=0.2492\n",
      "Epoch 415, val_loss=0.2435\n",
      "Epoch 416, val_loss=0.2302\n",
      "Epoch 417, val_loss=0.2452\n",
      "Epoch 418, val_loss=0.2531\n",
      "Epoch 419, val_loss=0.2381\n",
      "Epoch 420, val_loss=0.2453\n",
      "Epoch 421, val_loss=0.2509\n",
      "Epoch 422, val_loss=0.2474\n",
      "Epoch 423, val_loss=0.2563\n",
      "Epoch 424, val_loss=0.2487\n",
      "Epoch 425, val_loss=0.2403\n",
      "Epoch 426, val_loss=0.2482\n",
      "Epoch 427, val_loss=0.3450\n",
      "Epoch 428, val_loss=0.2502\n",
      "Epoch 429, val_loss=0.2376\n",
      "Epoch 430, val_loss=0.2446\n",
      "Early stopping triggered!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/08 11:32:47 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒ View run orderly-midge-973 at: http://127.0.0.1:5000/#/experiments/195097301821250000/runs/67ee5baf140442c79e333a1b6e1b688b\n",
      "ğŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/195097301821250000\n"
     ]
    }
   ],
   "source": [
    "X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test_t  = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_t  = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "\n",
    "val_split = int(0.2 * len(train_ds))\n",
    "train_subset, val_subset = random_split(train_ds, [len(train_ds)-val_split, val_split])\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=32)\n",
    "\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = RegressionNN(X_train_scaled.shape[1])    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.best_state = None\n",
    "\n",
    "    def step(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "early_stopping = EarlyStopping(patience=20)\n",
    "\n",
    "# --- Training Loop ---\n",
    "with mlflow.start_run():\n",
    "    for epoch in range(500):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                pred = model(xb)\n",
    "                val_losses.append(criterion(pred, yb).item())\n",
    "        val_loss = sum(val_losses) / len(val_losses)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, val_loss={val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if early_stopping.step(val_loss, model):\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    # Restore best weights\n",
    "    model.load_state_dict(early_stopping.best_state)\n",
    "\n",
    "    # Predict on test data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test_t).numpy()\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    mlflow.log_param(\"model_name\", \"Pytorch NN\")\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "    # Save model\n",
    "    mlflow.pytorch.log_model(model, name = model_name, input_example = X_train_scaled.astype(\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83e56296-c70a-4d86-9ec9-75dd4c0189bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 11:33:24.111259: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 593.9142 - val_loss: 514.0627\n",
      "Epoch 2/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 549.7261 - val_loss: 467.9417\n",
      "Epoch 3/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 494.0899 - val_loss: 408.0708\n",
      "Epoch 4/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 421.3989 - val_loss: 330.7395\n",
      "Epoch 5/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 330.2480 - val_loss: 239.9634\n",
      "Epoch 6/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 226.0311 - val_loss: 148.9106\n",
      "Epoch 7/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 132.4970 - val_loss: 79.3788\n",
      "Epoch 8/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 68.3092 - val_loss: 48.5525\n",
      "Epoch 9/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 43.3613 - val_loss: 39.4992\n",
      "Epoch 10/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 35.7573 - val_loss: 34.5419\n",
      "Epoch 11/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 31.3463 - val_loss: 30.0991\n",
      "Epoch 12/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 27.9967 - val_loss: 27.3810\n",
      "Epoch 13/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 25.7538 - val_loss: 25.6927\n",
      "Epoch 14/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 24.2563 - val_loss: 24.5603\n",
      "Epoch 15/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 23.0733 - val_loss: 23.9574\n",
      "Epoch 16/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 22.0498 - val_loss: 23.0639\n",
      "Epoch 17/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 21.1660 - val_loss: 22.4937\n",
      "Epoch 18/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 20.4327 - val_loss: 21.5039\n",
      "Epoch 19/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.7557 - val_loss: 21.1020\n",
      "Epoch 20/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.0230 - val_loss: 20.5879\n",
      "Epoch 21/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.5144 - val_loss: 20.0769\n",
      "Epoch 22/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 17.8785 - val_loss: 19.2312\n",
      "Epoch 23/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 17.3981 - val_loss: 18.8399\n",
      "Epoch 24/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 16.8501 - val_loss: 18.2563\n",
      "Epoch 25/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 16.3514 - val_loss: 18.1748\n",
      "Epoch 26/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 15.7900 - val_loss: 17.3956\n",
      "Epoch 27/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 15.3311 - val_loss: 16.9034\n",
      "Epoch 28/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 14.8943 - val_loss: 16.5475\n",
      "Epoch 29/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14.4912 - val_loss: 16.1853\n",
      "Epoch 30/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14.0270 - val_loss: 15.6292\n",
      "Epoch 31/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 13.6088 - val_loss: 15.3768\n",
      "Epoch 32/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 13.2411 - val_loss: 14.8402\n",
      "Epoch 33/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 12.8437 - val_loss: 14.3987\n",
      "Epoch 34/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 12.5456 - val_loss: 13.9922\n",
      "Epoch 35/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 12.1884 - val_loss: 13.7916\n",
      "Epoch 36/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 12.0396 - val_loss: 13.4774\n",
      "Epoch 37/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11.6752 - val_loss: 13.0249\n",
      "Epoch 38/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11.3366 - val_loss: 12.9690\n",
      "Epoch 39/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11.0431 - val_loss: 12.5671\n",
      "Epoch 40/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10.8542 - val_loss: 12.2663\n",
      "Epoch 41/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10.5719 - val_loss: 11.9119\n",
      "Epoch 42/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 10.3616 - val_loss: 11.7998\n",
      "Epoch 43/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 10.1910 - val_loss: 11.5842\n",
      "Epoch 44/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9.9973 - val_loss: 11.3072\n",
      "Epoch 45/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.8176 - val_loss: 11.0195\n",
      "Epoch 46/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.6500 - val_loss: 10.8260\n",
      "Epoch 47/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.4636 - val_loss: 10.8278\n",
      "Epoch 48/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.3358 - val_loss: 10.5982\n",
      "Epoch 49/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.1955 - val_loss: 10.4138\n",
      "Epoch 50/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.0817 - val_loss: 10.1783\n",
      "Epoch 51/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.9353 - val_loss: 10.0017\n",
      "Epoch 52/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.9086 - val_loss: 9.8878\n",
      "Epoch 53/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.7683 - val_loss: 9.5891\n",
      "Epoch 54/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.6055 - val_loss: 9.6010\n",
      "Epoch 55/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.5008 - val_loss: 9.3675\n",
      "Epoch 56/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.4221 - val_loss: 9.2177\n",
      "Epoch 57/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8.3776 - val_loss: 9.1795\n",
      "Epoch 58/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8.2793 - val_loss: 9.1216\n",
      "Epoch 59/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.1634 - val_loss: 8.9632\n",
      "Epoch 60/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.1058 - val_loss: 8.8682\n",
      "Epoch 61/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 8.0447 - val_loss: 8.7162\n",
      "Epoch 62/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.9756 - val_loss: 8.6425\n",
      "Epoch 63/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.8848 - val_loss: 8.5900\n",
      "Epoch 64/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.8842 - val_loss: 8.4783\n",
      "Epoch 65/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.7999 - val_loss: 8.4276\n",
      "Epoch 66/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.7465 - val_loss: 8.3145\n",
      "Epoch 67/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.6827 - val_loss: 8.1750\n",
      "Epoch 68/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 7.7347 - val_loss: 8.1949\n",
      "Epoch 69/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.6843 - val_loss: 8.1015\n",
      "Epoch 70/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.5584 - val_loss: 7.9146\n",
      "Epoch 71/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.5889 - val_loss: 7.9666\n",
      "Epoch 72/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.8107 - val_loss: 7.7141\n",
      "Epoch 73/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.5768 - val_loss: 8.0047\n",
      "Epoch 74/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.5085 - val_loss: 7.7466\n",
      "Epoch 75/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.4161 - val_loss: 7.6325\n",
      "Epoch 76/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.3082 - val_loss: 7.7514\n",
      "Epoch 77/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.2677 - val_loss: 7.5466\n",
      "Epoch 78/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.2059 - val_loss: 7.6187\n",
      "Epoch 79/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.1412 - val_loss: 7.4362\n",
      "Epoch 80/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.1220 - val_loss: 7.4116\n",
      "Epoch 81/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.1429 - val_loss: 7.3324\n",
      "Epoch 82/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.1735 - val_loss: 7.3202\n",
      "Epoch 83/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7.1123 - val_loss: 7.2922\n",
      "Epoch 84/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6.9422 - val_loss: 7.2172\n",
      "Epoch 85/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.9870 - val_loss: 7.0473\n",
      "Epoch 86/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.8878 - val_loss: 7.1069\n",
      "Epoch 87/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.8620 - val_loss: 6.9574\n",
      "Epoch 88/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.7712 - val_loss: 6.8645\n",
      "Epoch 89/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.7627 - val_loss: 6.8512\n",
      "Epoch 90/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.7967 - val_loss: 6.7419\n",
      "Epoch 91/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6244 - val_loss: 6.8238\n",
      "Epoch 92/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.6262 - val_loss: 6.7084\n",
      "Epoch 93/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.5384 - val_loss: 6.5044\n",
      "Epoch 94/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.4799 - val_loss: 6.5912\n",
      "Epoch 95/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.4473 - val_loss: 6.5522\n",
      "Epoch 96/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.4441 - val_loss: 6.5079\n",
      "Epoch 97/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.3138 - val_loss: 6.2138\n",
      "Epoch 98/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.2519 - val_loss: 6.2673\n",
      "Epoch 99/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.2707 - val_loss: 6.1566\n",
      "Epoch 100/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.2279 - val_loss: 6.0964\n",
      "Epoch 101/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.0861 - val_loss: 6.0596\n",
      "Epoch 102/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.0183 - val_loss: 5.9302\n",
      "Epoch 103/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.9573 - val_loss: 5.8547\n",
      "Epoch 104/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.9108 - val_loss: 5.8009\n",
      "Epoch 105/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.9032 - val_loss: 5.7667\n",
      "Epoch 106/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.7564 - val_loss: 5.6805\n",
      "Epoch 107/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.6864 - val_loss: 5.5931\n",
      "Epoch 108/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.6522 - val_loss: 5.5898\n",
      "Epoch 109/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.5857 - val_loss: 5.5148\n",
      "Epoch 110/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.5268 - val_loss: 5.3834\n",
      "Epoch 111/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.4157 - val_loss: 5.4047\n",
      "Epoch 112/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3358 - val_loss: 5.3213\n",
      "Epoch 113/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.2793 - val_loss: 5.2099\n",
      "Epoch 114/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.2129 - val_loss: 5.1504\n",
      "Epoch 115/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5.1526 - val_loss: 5.1440\n",
      "Epoch 116/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.1006 - val_loss: 5.0183\n",
      "Epoch 117/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.0377 - val_loss: 5.0533\n",
      "Epoch 118/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.0244 - val_loss: 5.0474\n",
      "Epoch 119/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.0281 - val_loss: 4.8043\n",
      "Epoch 120/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.8383 - val_loss: 4.8380\n",
      "Epoch 121/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.7513 - val_loss: 4.7089\n",
      "Epoch 122/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.7158 - val_loss: 4.6459\n",
      "Epoch 123/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.6932 - val_loss: 4.6508\n",
      "Epoch 124/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.5477 - val_loss: 4.4920\n",
      "Epoch 125/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.4856 - val_loss: 4.4949\n",
      "Epoch 126/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.4631 - val_loss: 4.3767\n",
      "Epoch 127/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.4088 - val_loss: 4.3588\n",
      "Epoch 128/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.2969 - val_loss: 4.3365\n",
      "Epoch 129/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.2807 - val_loss: 4.1941\n",
      "Epoch 130/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.1838 - val_loss: 4.1179\n",
      "Epoch 131/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.0623 - val_loss: 4.0432\n",
      "Epoch 132/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.0640 - val_loss: 4.1072\n",
      "Epoch 133/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9762 - val_loss: 3.8862\n",
      "Epoch 134/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.8631 - val_loss: 3.8788\n",
      "Epoch 135/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7694 - val_loss: 3.7880\n",
      "Epoch 136/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7283 - val_loss: 3.7465\n",
      "Epoch 137/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.6202 - val_loss: 3.5674\n",
      "Epoch 138/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.5569 - val_loss: 3.5024\n",
      "Epoch 139/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4997 - val_loss: 3.6416\n",
      "Epoch 140/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4215 - val_loss: 3.4161\n",
      "Epoch 141/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4193 - val_loss: 3.4103\n",
      "Epoch 142/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2565 - val_loss: 3.2572\n",
      "Epoch 143/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.1658 - val_loss: 3.1930\n",
      "Epoch 144/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.0535 - val_loss: 3.1383\n",
      "Epoch 145/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9733 - val_loss: 3.0051\n",
      "Epoch 146/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9003 - val_loss: 2.9188\n",
      "Epoch 147/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.8299 - val_loss: 3.0157\n",
      "Epoch 148/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7483 - val_loss: 2.8210\n",
      "Epoch 149/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6573 - val_loss: 2.7356\n",
      "Epoch 150/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5947 - val_loss: 2.7571\n",
      "Epoch 151/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5096 - val_loss: 2.5704\n",
      "Epoch 152/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.4386 - val_loss: 2.5586\n",
      "Epoch 153/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3329 - val_loss: 2.4561\n",
      "Epoch 154/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2743 - val_loss: 2.3867\n",
      "Epoch 155/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.2614 - val_loss: 2.3590\n",
      "Epoch 156/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1567 - val_loss: 2.2414\n",
      "Epoch 157/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0352 - val_loss: 2.2026\n",
      "Epoch 158/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.9645 - val_loss: 2.0829\n",
      "Epoch 159/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8975 - val_loss: 1.9966\n",
      "Epoch 160/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8482 - val_loss: 1.9501\n",
      "Epoch 161/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7584 - val_loss: 1.9182\n",
      "Epoch 162/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7079 - val_loss: 1.9549\n",
      "Epoch 163/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6585 - val_loss: 1.8036\n",
      "Epoch 164/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5921 - val_loss: 1.9246\n",
      "Epoch 165/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5381 - val_loss: 1.6604\n",
      "Epoch 166/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4548 - val_loss: 1.7011\n",
      "Epoch 167/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4787 - val_loss: 1.5125\n",
      "Epoch 168/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4275 - val_loss: 1.5214\n",
      "Epoch 169/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.3384 - val_loss: 1.4270\n",
      "Epoch 170/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2207 - val_loss: 1.3711\n",
      "Epoch 171/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1798 - val_loss: 1.3022\n",
      "Epoch 172/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1232 - val_loss: 1.2275\n",
      "Epoch 173/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0863 - val_loss: 1.2238\n",
      "Epoch 174/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0455 - val_loss: 1.2074\n",
      "Epoch 175/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.0025 - val_loss: 1.1363\n",
      "Epoch 176/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9517 - val_loss: 1.1026\n",
      "Epoch 177/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9247 - val_loss: 1.0523\n",
      "Epoch 178/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9010 - val_loss: 1.0787\n",
      "Epoch 179/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8298 - val_loss: 0.9334\n",
      "Epoch 180/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7845 - val_loss: 0.8967\n",
      "Epoch 181/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7822 - val_loss: 0.8498\n",
      "Epoch 182/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7166 - val_loss: 0.8409\n",
      "Epoch 183/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6864 - val_loss: 0.8027\n",
      "Epoch 184/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6676 - val_loss: 0.7639\n",
      "Epoch 185/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6228 - val_loss: 0.7816\n",
      "Epoch 186/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6074 - val_loss: 0.7350\n",
      "Epoch 187/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5691 - val_loss: 0.7166\n",
      "Epoch 188/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5553 - val_loss: 0.7142\n",
      "Epoch 189/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5296 - val_loss: 0.6773\n",
      "Epoch 190/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5130 - val_loss: 0.6223\n",
      "Epoch 191/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5132 - val_loss: 0.5888\n",
      "Epoch 192/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4697 - val_loss: 0.5972\n",
      "Epoch 193/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4549 - val_loss: 0.5817\n",
      "Epoch 194/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4550 - val_loss: 0.5832\n",
      "Epoch 195/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4232 - val_loss: 0.5754\n",
      "Epoch 196/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4060 - val_loss: 0.5653\n",
      "Epoch 197/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4075 - val_loss: 0.5233\n",
      "Epoch 198/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4084 - val_loss: 0.5144\n",
      "Epoch 199/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3684 - val_loss: 0.4910\n",
      "Epoch 200/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3522 - val_loss: 0.4822\n",
      "Epoch 201/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3478 - val_loss: 0.4734\n",
      "Epoch 202/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3338 - val_loss: 0.4618\n",
      "Epoch 203/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3257 - val_loss: 0.4184\n",
      "Epoch 204/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3190 - val_loss: 0.4573\n",
      "Epoch 205/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3278 - val_loss: 0.4489\n",
      "Epoch 206/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3152 - val_loss: 0.4056\n",
      "Epoch 207/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2950 - val_loss: 0.4237\n",
      "Epoch 208/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3046 - val_loss: 0.4384\n",
      "Epoch 209/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3012 - val_loss: 0.4328\n",
      "Epoch 210/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2829 - val_loss: 0.3905\n",
      "Epoch 211/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2843 - val_loss: 0.3615\n",
      "Epoch 212/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2666 - val_loss: 0.3535\n",
      "Epoch 213/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2608 - val_loss: 0.3605\n",
      "Epoch 214/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2510 - val_loss: 0.3697\n",
      "Epoch 215/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2504 - val_loss: 0.3825\n",
      "Epoch 216/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2532 - val_loss: 0.3425\n",
      "Epoch 217/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2607 - val_loss: 0.3441\n",
      "Epoch 218/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2485 - val_loss: 0.3396\n",
      "Epoch 219/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2414 - val_loss: 0.3546\n",
      "Epoch 220/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2428 - val_loss: 0.3254\n",
      "Epoch 221/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2273 - val_loss: 0.3228\n",
      "Epoch 222/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2381 - val_loss: 0.3339\n",
      "Epoch 223/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2236 - val_loss: 0.3083\n",
      "Epoch 224/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2168 - val_loss: 0.2969\n",
      "Epoch 225/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2235 - val_loss: 0.2951\n",
      "Epoch 226/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2230 - val_loss: 0.3213\n",
      "Epoch 227/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2146 - val_loss: 0.2893\n",
      "Epoch 228/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2149 - val_loss: 0.2755\n",
      "Epoch 229/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2225 - val_loss: 0.2909\n",
      "Epoch 230/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2061 - val_loss: 0.2921\n",
      "Epoch 231/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2017 - val_loss: 0.2675\n",
      "Epoch 232/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2162 - val_loss: 0.3388\n",
      "Epoch 233/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2104 - val_loss: 0.2727\n",
      "Epoch 234/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2121 - val_loss: 0.2897\n",
      "Epoch 235/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1962 - val_loss: 0.2598\n",
      "Epoch 236/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1929 - val_loss: 0.2839\n",
      "Epoch 237/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1984 - val_loss: 0.2880\n",
      "Epoch 238/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2080 - val_loss: 0.2585\n",
      "Epoch 239/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1922 - val_loss: 0.2807\n",
      "Epoch 240/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1913 - val_loss: 0.2669\n",
      "Epoch 241/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1880 - val_loss: 0.3317\n",
      "Epoch 242/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2398 - val_loss: 0.2837\n",
      "Epoch 243/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2170 - val_loss: 0.2861\n",
      "Epoch 244/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1838 - val_loss: 0.2530\n",
      "Epoch 245/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1843 - val_loss: 0.2843\n",
      "Epoch 246/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1827 - val_loss: 0.2687\n",
      "Epoch 247/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1923 - val_loss: 0.2255\n",
      "Epoch 248/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1796 - val_loss: 0.2404\n",
      "Epoch 249/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1725 - val_loss: 0.2455\n",
      "Epoch 250/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1731 - val_loss: 0.2374\n",
      "Epoch 251/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1729 - val_loss: 0.2445\n",
      "Epoch 252/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1845 - val_loss: 0.2380\n",
      "Epoch 253/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1709 - val_loss: 0.2654\n",
      "Epoch 254/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1660 - val_loss: 0.2556\n",
      "Epoch 255/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1645 - val_loss: 0.2283\n",
      "Epoch 256/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1642 - val_loss: 0.2507\n",
      "Epoch 257/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1705 - val_loss: 0.2549\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/08 11:34:17 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "ğŸƒ View run persistent-shad-523 at: http://127.0.0.1:5000/#/experiments/195097301821250000/runs/6427899293a74a329c87699426b2703c\n",
      "ğŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/195097301821250000\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.Input(shape=(X_train_scaled.shape[1],)),\n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    keras.layers.Dense(32, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)   # regression output\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=\"mse\"\n",
    ")\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model.fit(X_train_scaled, y_train, epochs=500, batch_size=32, validation_split=0.2, verbose = 1, callbacks=[early_stopping])\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    mlflow.log_param(\"model_name\", \"Neural Networks\")\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.sklearn.log_model(model, name = model_name, input_example = X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5242bbd1-82fc-4a16-9e5b-2f30605341e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Run ID: 6427899293a74a329c87699426b2703c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'heat-forecast-prod' already exists. Creating a new version of this model...\n",
      "2025/12/08 11:35:58 WARNING mlflow.tracking._model_registry.fluent: Run with id 6427899293a74a329c87699426b2703c has no artifacts at artifact path 'heat-forecast-prod', registering model based on models:/m-23fa2c90c13640d0812a0a7ee0cbadba instead\n",
      "2025/12/08 11:35:58 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: heat-forecast-prod, version 4\n",
      "Created version '4' of model 'heat-forecast-prod'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸƒ View run persistent-shad-523 at: http://127.0.0.1:5000/#/experiments/195097301821250000/runs/6427899293a74a329c87699426b2703c\n",
      "ğŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/195097301821250000\n"
     ]
    }
   ],
   "source": [
    "# Register model\n",
    "run_id = input('Run ID:')\n",
    "model_uri = f'runs:/{run_id}/{model_name}'\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    mlflow.register_model(model_uri=model_uri, name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02e5f873-ab08-42bd-80c9-d73b9ba07d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:[15.09619  13.24421  32.464813 41.37811 ]\n",
      "Test:[16.47 13.17 32.82 41.32]\n"
     ]
    }
   ],
   "source": [
    "model_uri = f\"models:/{model_name}@champion\"\n",
    "loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "y_pred = loaded_model.predict(X_test_scaled)\n",
    "print(\"Prediction:\", end=\"\")\n",
    "print(y_pred[:4])\n",
    "print(\"Test:\", end=\"\")\n",
    "print(y_test[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e169d68-0f42-49b9-bd4f-bd18e39ded4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
